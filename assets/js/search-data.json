{
  
    
        "post0": {
            "title": "Work in Progress: Recreating Fast.ai Chapter 5 Using TensorFlow/Keras",
            "content": "WIP . This post is currently a work in progress and will be progressively updated as I work on it. . Started: 7/21/21. | Last updated: 7/24/21. | . TODO . Update to use fastcore | Do 2 stage training | Try crop resizing vs squish | . Later TODOs . Normal data aug -&gt; Presizing | lr Finder | 1cycle lr | Discriminative lrs | Confusion matrix, most confused | Mixed precision | Comparison to Fast.ai | . Image Classification . import tensorflow as tf import numpy as np import random import matplotlib.pyplot as plt from PIL import Image from pathlib import Path import re from fastcore.all import * from tensorflow.keras import Input, Model, Sequential from tensorflow.keras.layers import Dense, GlobalAvgPool2D from tensorflow.keras.optimizers import Adam from tensorflow.keras.losses import SparseCategoricalCrossentropy . seed = 42 random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) . From Dogs and Cats to Pet Breeds . Let&#39;s use tf.data API to create datasets. We could have used tf.keras.preprocessing.image_dataset_from_directory, but that would have required us to change our file structure where our images are, which I don&#39;t really want to do because the Fast.ai lessons depend on it having that file structure. . Getting Files and Class Codes(?) . path = Path(&quot;/home/brandon/.fastai/data/oxford-iiit-pet/images/&quot;) . files = L(path.glob(r&quot;*.jpg&quot;)); files . (#7390) [Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/Birman_115.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/leonberger_142.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/Bombay_68.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/japanese_chin_26.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/saint_bernard_149.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/Ragdoll_41.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/japanese_chin_32.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/Ragdoll_68.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/Persian_202.jpg&#39;),Path(&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/scottish_terrier_143.jpg&#39;)...] . Image.open(files[0]) . lbls = L(re.findall(f&quot;^(.+)_ d+$&quot;, f.stem)[0] for f in files); lbls . (#7390) [&#39;Birman&#39;,&#39;leonberger&#39;,&#39;Bombay&#39;,&#39;japanese_chin&#39;,&#39;saint_bernard&#39;,&#39;Ragdoll&#39;,&#39;japanese_chin&#39;,&#39;Ragdoll&#39;,&#39;Persian&#39;,&#39;scottish_terrier&#39;...] . vocab = np.array(lbls.unique(sort=True)); vocab . array([&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;], dtype=&#39;&lt;U26&#39;) . Awesome, now we&#39;re ready to create out tf.data.Dataset. We have to convert Path objects to strings. This part was analogous to the get_items and get_x and get_y arguments to fastai&#39;s DataBlock. . data = tf.data.Dataset.from_tensor_slices((files.map(str).items, lbls.items)); data, next(iter(data)) . (&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;, (&lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/Birman_115.jpg&#39;&gt;, &lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;Birman&#39;&gt;)) . Shuffle before splitting val and train. Make sure not to reshuffle each iteration to avoid mixing val and train on different epochs. . data = data.shuffle(len(data), reshuffle_each_iteration=False) . Splitting into Train and Val . Since our examples are shuffled randomly, we can just take the first 20% of images to be the validation set. The final 80% will be the train set. . num_val = int(len(data) * 0.2) # 20% for validation set train_ds, val_ds = data.skip(num_val), data.take(num_val) len(train_ds), len(val_ds) . (5912, 1478) . assert len(train_ds) + len(val_ds) == len(data) # Check no examples were lost assert not set(f.numpy() for f,lbl in train_ds) &amp; set(f.numpy() for f,lbl in val_ds) # check that sets are disjoint . Transform our independent and dependent variables into something we can feed into model . Now we transform our data into something we can feed into our model. This part is analogous to the DataBlock API&#39;s item_tfms and batch_tfms. It also will do the role of creating a DataLoader. In TensorFlow, there are not separate Dataset and DataLoader abstractions, your tf.data.Dataset object does it all :). . x, y = next(iter(data)); x, y . (&lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;/home/brandon/.fastai/data/oxford-iiit-pet/images/Sphynx_95.jpg&#39;&gt;, &lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;Sphynx&#39;&gt;) . TODO: Don&#39;t hardcode this, pass to dataset getter function . Transforms . IMG_SIZE = 224 AUTOTUNE = tf.data.AUTOTUNE . def decode_image(filename): &quot;&quot;&quot;Reads image as tensor and resizes.&quot;&quot;&quot; raw_bytes = tf.io.read_file(filename) img = tf.io.decode_jpeg(raw_bytes, channels=3) return tf.image.resize(img, [IMG_SIZE, IMG_SIZE]) # TODO: this changes aspect ratio, experiment w/crop resizing . img = decode_image(x) assert img.shape[-1] == 3 assert img.dtype == tf.float32 . def encode_class(label): &quot;&quot;&quot;Converts class from string tensor -&gt; integer.&quot;&quot;&quot; return tf.argmax(label == vocab) . lbl = encode_class(y) assert lbl.shape == () assert lbl.dtype == tf.int64 # can we encode this as smaller dtype? . @tf.function def decode_example(filename, label): return decode_image(filename), encode_class(label) . train_ds = train_ds.map(decode_example, num_parallel_calls=AUTOTUNE); train_ds . &lt;ParallelMapDataset shapes: ((224, 224, 3), ()), types: (tf.float32, tf.int64)&gt; . Doing those were like doing the item_tfms of Fast.ai. That read in the image and label as numeric tensors and resized the images all to the same size. Now we&#39;ll be able to batch them together. . Putting it all Together . def prep_dataset(ds: tf.data.Dataset, batch_size: int, shuffle: bool): if shuffle: ds = ds.shuffle(len(ds)) ds = ds.map(decode_example, num_parallel_calls=AUTOTUNE) ds = ds.batch(batch_size, num_parallel_calls=AUTOTUNE) return ds.prefetch(AUTOTUNE) . def get_datasets(files: list[str], labels: list[str], batch_size: int, shuffle: bool = True): data = tf.data.Dataset.from_tensor_slices((files, labels)) # Shuffle before splitting off validation set data = data.shuffle(len(data), reshuffle_each_iteration=False) # Split into train and val sets num_val = int(len(data) * 0.2) # Take 20% for validation set train_ds, val_ds = data.skip(num_val), data.take(num_val) return prep_dataset(train_ds, batch_size, shuffle=shuffle), prep_dataset(val_ds, batch_size, shuffle=False) . train_ds, val_ds = get_datasets(list(files.map(str)), list(lbls), batch_size=128, shuffle=True) . Visualize Data . xb, yb = next(iter(train_ds)) xb.shape, yb.shape . (TensorShape([128, 224, 224, 3]), TensorShape([128])) . fig = plt.figure(figsize=(20, 12)) for i, (x, y) in enumerate(zip(xb[:9], yb[:9])): ax = plt.subplot(3, 3, i + 1) ax.imshow(tf.cast(x, tf.uint8)) ax.set_title(vocab[y.numpy()]) . Build Model . We&#39;re going to use a pretrained ResNet50. . def get_model(): inp = Input(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32) x = tf.keras.applications.resnet50.preprocess_input(inp) x = tf.keras.applications.ResNet50(include_top=False, weights=&quot;imagenet&quot;)(x) x = GlobalAvgPool2D()(x) x = Dense(len(vocab))(x) return Model(inp, x) . model = get_model() model.compile(optimizer=Adam(learning_rate=1e-3), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=&quot;accuracy&quot;) model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ tf.__operators__.getitem_1 ( (None, 224, 224, 3) 0 _________________________________________________________________ tf.nn.bias_add_1 (TFOpLambda (None, 224, 224, 3) 0 _________________________________________________________________ resnet50 (Functional) (None, None, None, 2048) 23587712 _________________________________________________________________ global_average_pooling2d_1 ( (None, 2048) 0 _________________________________________________________________ dense_1 (Dense) (None, 37) 75813 ================================================================= Total params: 23,663,525 Trainable params: 23,610,405 Non-trainable params: 53,120 _________________________________________________________________ . Training . hist = model.fit(train_ds, epochs=10, validation_data=val_ds) . Epoch 1/10 47/47 [==============================] - 24s 375ms/step - loss: 1.3151 - accuracy: 0.6160 - val_loss: 6.1195 - val_accuracy: 0.0947 Epoch 2/10 47/47 [==============================] - 16s 331ms/step - loss: 0.5291 - accuracy: 0.8353 - val_loss: 2.8901 - val_accuracy: 0.4641 Epoch 3/10 47/47 [==============================] - 16s 333ms/step - loss: 0.2684 - accuracy: 0.9151 - val_loss: 2.2679 - val_accuracy: 0.4926 Epoch 4/10 47/47 [==============================] - 16s 334ms/step - loss: 0.1857 - accuracy: 0.9410 - val_loss: 1.4779 - val_accuracy: 0.6008 Epoch 5/10 47/47 [==============================] - 16s 334ms/step - loss: 0.1174 - accuracy: 0.9640 - val_loss: 2.4915 - val_accuracy: 0.5900 Epoch 6/10 47/47 [==============================] - 16s 334ms/step - loss: 0.1928 - accuracy: 0.9408 - val_loss: 2.4740 - val_accuracy: 0.4811 Epoch 7/10 47/47 [==============================] - 16s 334ms/step - loss: 0.2018 - accuracy: 0.9337 - val_loss: 1.7589 - val_accuracy: 0.6116 Epoch 8/10 47/47 [==============================] - 16s 336ms/step - loss: 0.1088 - accuracy: 0.9660 - val_loss: 1.1136 - val_accuracy: 0.7118 Epoch 9/10 47/47 [==============================] - 16s 335ms/step - loss: 0.0620 - accuracy: 0.9816 - val_loss: 1.0387 - val_accuracy: 0.7510 Epoch 10/10 47/47 [==============================] - 16s 335ms/step - loss: 0.0643 - accuracy: 0.9816 - val_loss: 1.2890 - val_accuracy: 0.6928 . &lt;tensorflow.python.keras.callbacks.History at 0x7fccc8047f40&gt; . Model Interpretation . Improving Our Model . The Learning Rate Finder . Unfreezing and Transfer Learning . Discriminative Learning Rates . Selecting the Number of Epochs . Deeper Architectures .",
            "url": "https://brandonwolfson.com/fastai/tensorflow/keras/2021/07/22/fastbook-ch-5-tf.html",
            "relUrl": "/fastai/tensorflow/keras/2021/07/22/fastbook-ch-5-tf.html",
            "date": " • Jul 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "The Importance of Validation and Test Sets",
            "content": "So you’re studying machine learning and keep hearing about validation and test sets. You know they’re important, but you’re not sure why. That’s what this article is about. Let’s dive in! . . Why are Validation and Test Sets Important? . Here’s the big picture: Validation sets help you prevent overfitting and test sets tell you if your model will generalize to new data. Let’s unpack that. Overfitting is when your model performs well on the data it trained on, but poorly on new data. A model is said to generalize when it performs similarly on both training data and new data. . The whole point of training a model is to use it on new data (if we made an animal classifier that only worked on animals the model had previously seen, it wouldn’t be very useful!). We want a model that generalizes, which means avoiding overfitting is a key concern. But why does overfitting even happen in the first place? . Why Does Overfitting Occur? . When our model trains, it’s just learning to map inputs to outputs in the train set. Initially, it doesn’t know anything and makes terrible predictions. After seeing the data a few times though, it learns to pick out general features in the inputs that help it predict the outputs. Finally, with enough passes through the data, the model learns to map perfectly. It no longer has to rely on the general features it found earlier. Instead, it now knows exactly what each input in the training data looks like, and has memorized its corresponding output. Great, right? . Not quite! The problem here is that our goal is different than the model’s goal! Our goal is to have a model that generalizes to new data, but the model’s goal is to perform perfectly on the training data! In other words, the model’s goal is to overfit! It doesn’t care about or even know about other data. . . So how do we stop our model from overfitting? If we had another set of data that the model didn’t train on, we could periodically test its performance on that data. That would let us find the point when the model stopped learning general features (which help classify new data) and when the model started memorizing its training data. That’s exactly what a validation set is! . Validation Sets Prevent Overfitting on the Training Set . By testing our model’s performance on a validation set after each epoch, we’re able to see when it starts to get worse. Right before this happens is when our model can generalize to new data the best! Not only that, we can use our validation set to adjust our other hyperparameters (learning rate, regularization, architecture, etc.), not just the number of training epochs, to get a model that can perform even better on the validation set! So a validation set gives us two key advantages: . We can see when our model stops generalizing and starts overfitting | We can tune the model’s hyperparameters so that it achieves better performance on the validation set before #1 happens | Great, we’ve used our validation set to tune our hyperparameters. Now we know our model will generalize the best, right? . Not quite! We can’t be sure how our model will perform on new data yet. Why? Your model tuned its parameters using feedback it got from the train set. This presented the opportunity for the model to overfit to the train set. But you tuned the model’s hyperparameters using feedback from the validation set. This means that you may have unknowingly caused the model to overfit to the validation set. The model might just be tuned to perform well on the validation set, but might not generalize to completely new data! . . That’s where the test set comes in. . The Test Set Tells if You Overfit on the Validation Set . By testing the model’s performance on truly unseen data once we’re completely done tuning it, we get an accurate estimate of how it will perform in production! . 2 Interesting Implications . We could still get an accurate estimate of how our model would perform in production without a validation set, using only a train and test set. However, this would be bad because we’d lose the two advantages of the validation set. We wouldn’t know if our model was overfitting on the train set, which means any hyperparameter tuning would be meaningless. We’d have no way to tell if it was making the model more general, or just making it overfit more. | You can only test on the test set once! If you test your model on the test set and then go back and tune it to perform better, the test set becomes the same as the validation set! You won’t be able to tell if any increase in performance is because of a more general model or because of overfitting to the test set! That’s why Kaggle has the private leaderboard. Since they allow multiple submissions, people can tune their model to perform better on the public leaderboard. This introduces the potential of overfitting to it. By withholding some of the test data for the private leaderboard, Kaggle can see which models generalize the best. | How Should You Pick Your Validation and Test Sets? . The way you pick what data goes into your validation and test sets is extremely important. If the data your model encounters in production is in some way fundamentally different than the data you had in your test set, then your test set won’t provide an accurate estimate of your model’s performance. The main principle to keep in mind is this: Pick data that are as similar as possible to the type of data your model will see in production (while preventing any overlap in data between the train, validation, and test sets). For a great write-up on this, check out Fast.ai co-founder Rachel Thomas’s articlehere. . Enjoy the article? Let me know your thoughts in the comments below! :) .",
            "url": "https://brandonwolfson.com/2020/08/17/the-importance-of-validation-and-test-sets.html",
            "relUrl": "/2020/08/17/the-importance-of-validation-and-test-sets.html",
            "date": " • Aug 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I recently graduated from UCLA with a B.S. in Mathematics of Computation. I’m now a software engineer specializing in machine learning applications at Centauri. I’m interested in applying machine learning (and specifically deep learning) to practical problems. . Outside of programming, you can catch me performing improv comedy, reading nonfiction, or lifting weights. :) .",
          "url": "https://brandonwolfson.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "Here is a sampling of some projects I’ve done outside of work. Enjoy! :) . Plant Pathology Classifier . . I built and deployed a neural network that classifies diseases of plants based on images of their leaves. I did this project because I’ve been learning Fast.ai and PyTorch in my free time and wanted to apply my knowledge somewhere. This data was from the Kaggle Plant Pathology 2020 - FGVC7 competition. I then deployed my classifier to the web using a simple web app I created with Streamlit. . Modeling COVID-19 Prevalence in US Prisons . . In this project, I used Python, the Epidemics on Networks (EoN) Python package, and Jupyter Notebooks to explore the effects of various interventions on the spread of COVID-19 in US prisons. This project was done as the final project of my UCLA Introduction to Networks course. It was a group project, with my group being me, Marcel Burgunder, Emma Broback, and Sam Briante. I did most of the coding and some writing. Marcel did some coding and lots of writing. Read our results here. .",
          "url": "https://brandonwolfson.com/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://brandonwolfson.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}